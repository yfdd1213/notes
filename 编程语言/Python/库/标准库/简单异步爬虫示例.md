##### 简单异步爬虫示例
```python
import asyncio
import aiohttp

async def fetch_url(url):  # 协程函数
    async with aiohttp.ClientSession() as session:  # 会话
        async with session.get(url) as response:  # 访问网页
            return await response.text()  # 等待文本

async def main():
    urls = [
        'https://www.example.com/page1',
        'https://www.example.com/page2',
        'https://www.example.com/page3'
    ]

    tasks = [fetch_url(url) for url in urls]
    results = await asyncio.gather(*tasks)  # 并行访问任务

    for url, content in zip(urls, results):
        print(f"Content of {url}: {len(content)} characters")

asyncio.run(main())  # 入口

```